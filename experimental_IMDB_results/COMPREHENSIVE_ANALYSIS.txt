====================================================================================================
COMP576 PEFT PROJECT - COMPREHENSIVE EXPERIMENTAL RESULTS AND ANALYSIS
====================================================================================================
Student: Haotian Xia (Prompt Tuning Implementation)
Date: 2025-12-04 21:04:53
====================================================================================================

EXPERIMENTAL CONFIGURATION:
--------------------------------------------------
Model: BERT-base-uncased (110M parameters)
Dataset: IMDB Movie Reviews (Binary Sentiment Classification)
  - Training samples: 25,000
  - Test samples: 25,000
Training Configuration:
  - Epochs: 3
  - Batch Size: 32
  - Gradient Accumulation: 4
  - Seed: 42
Methods Tested:
  - Full Fine-Tuning: All 110M parameters trainable
  - LoRA: Ranks tested = [8, 16, 32, 64, 128]
  - Prompt Tuning: Lengths tested = [10, 20, 50] virtual tokens

====================================================================================================
TABLE 1: FULL FINE-TUNING BASELINE
====================================================================================================
Method               Accuracy     F1 Score     Time (s)     Parameters     
--------------------------------------------------------------------------------
Full Fine-Tuning     0.9204       0.9212       196.5       ~110,000,000

====================================================================================================
TABLE 2: LORA ABLATION STUDY
====================================================================================================
Rank     Accuracy     F1 Score     Time (s)     Parameters      % of Full   
------------------------------------------------------------------------------------------
8        0.8080       0.8160       183.7       296,450         0.270%
16       0.8510       0.8529       174.6       591,362         0.538%
32       0.8691       0.8710       167.7       1,181,186       1.074%
64       0.8814       0.8832       168.3       2,360,834       2.146%
128      0.8897       0.8917       186.7       4,720,130       4.291%

====================================================================================================
TABLE 3: PROMPT TUNING ABLATION STUDY (HAOTIAN XIA'S WORK)
====================================================================================================
Length   Accuracy     F1 Score     Time (s)     Parameters      % of Full   
------------------------------------------------------------------------------------------
10       0.8776       0.8759       167.1       9,218           0.0084%
20       0.8825       0.8824       164.0       16,898          0.0154%
50       0.8781       0.8823       177.8       39,938          0.0363%

====================================================================================================
DETAILED ANALYSIS
====================================================================================================

## 1. PERFORMANCE COMPARISON ##

Best performing methods:
  Baseline: Full Fine-Tuning - 92.04%
  LoRA: Rank 128 - 88.97%
  Prompt Tuning: Length 20 - 88.25%

## 2. LORA RANK ANALYSIS ##

Rank vs Performance:
  Rank   8: Acc 80.80%, Params  296,450, Time 183.7s
  Rank  16: Acc 85.10%, Params  591,362, Time 174.6s
  Rank  32: Acc 86.91%, Params 1,181,186, Time 167.7s
  Rank  64: Acc 88.14%, Params 2,360,834, Time 168.3s
  Rank 128: Acc 88.97%, Params 4,720,130, Time 186.7s

## 3. PROMPT TUNING LENGTH ANALYSIS ##

Length vs Performance:
  Length 10: Acc 87.76%, Params  9,218, Time 167.1s
  Length 20: Acc 88.25%, Params 16,898, Time 164.0s
  Length 50: Acc 87.81%, Params 39,938, Time 177.8s

## 4. KEY FINDINGS ##

Parameter Efficiency:
  - Prompt Tuning achieves excellent efficiency with minimal parameters
  - LoRA provides flexible accuracy-parameter tradeoffs
  - Diminishing returns observed at higher ranks/lengths

Practical Recommendations:
  - For maximum accuracy: Use Full Fine-Tuning
  - For best efficiency: Use Prompt Tuning (length 20-50)
  - For balanced tradeoff: Use LoRA (rank 32-64)
  - For multi-task scenarios: Prompt Tuning (minimal storage per task)

====================================================================================================
